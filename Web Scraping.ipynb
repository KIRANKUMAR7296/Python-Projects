{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Stories from **Google News** by Extracting all **Tags** from the **HTML** of **Google News**. \n",
    "\n",
    "**Google News** used **Tags** to Create **Links** to various Websites.\n",
    "\n",
    "pip install **beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\_\\_init\\_\\_** method uses a website to extract as a **Parameter**\n",
    "\n",
    "**urlopen** function sends a request to a website and returns a Response Object in which **HTML** Code is stored.\n",
    "\n",
    "**read** returns the **HTML** of Response Object.\n",
    "\n",
    "**BeautifulSoup** parses the **HTML**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "    \n",
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        string = BeautifulSoup(html, parser)\n",
    "        \n",
    "        for tag in string.find_all(\"a\"):\n",
    "            url = tag.get(\"href\")\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"articles\" in url:\n",
    "                print(\"\\n\" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"https://news.google.com/\"\n",
    "# Scraper(news).scrape()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping \n",
    "\n",
    "- Some website offer **Data Sets** are Downloadable in **CSV** Format\n",
    "\n",
    "- Accessible via an Application Programming Interface **API**\n",
    "\n",
    "- But many Websites with confidential Data don't offer these convenient options.\n",
    "\n",
    "### How does Web Scraping Work?\n",
    "\n",
    "- We write code that sends a **Request** to the **Server** that's hosting the page.\n",
    "\n",
    "- Code downloads that page's source code.\n",
    "\n",
    "- But instead of Displaying the Page visually, it **Filters** through the page looking for HTML elements.\n",
    "\n",
    "- Consider **Caching** the content you Scrape so that it's only Downloaded once.\n",
    "\n",
    "### Components of Web Page\n",
    "\n",
    "When we visit a **Web Page**, our **Web Browser** makes a request to a **Web Server**.\n",
    "\n",
    "The Request is called a **GET** request\n",
    "\n",
    "1. HTML : Main **Content** of the page.\n",
    "2. CSS : Add **Styling** to make page look good.\n",
    "3. JavaScript : Add **Interactivity** to Web Pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n    </body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "print(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing a page with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   Here is some simple content for this page.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# Format \n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['html', '\\n', <html>\n",
      "<head>\n",
      "<title>A simple example page</title>\n",
      "</head>\n",
      "<body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>\n",
      "</html>]\n"
     ]
    }
   ],
   "source": [
    "print(list(soup.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'bs4.element.Doctype'>, <class 'bs4.element.NavigableString'>, <class 'bs4.element.Tag'>]\n"
     ]
    }
   ],
   "source": [
    "print([type(item) for item in list(soup.children)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A simple example page</title>\n",
      "</head>\n",
      "<body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "html = list(soup.children)[2]\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<p>Here is some simple content for this page.</p>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "body = list(html.children)[3]\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "p = list(body.children)[1]\n",
    "print(p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    First paragraph.\n",
      "   </p>\n",
      "   <p class=\"inner-text\">\n",
      "    Second paragraph.\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    First outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    Second outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                First outer paragraph.\n",
      "            </b>\n",
      "</p>, <p class=\"outer-text\">\n",
      "<b>\n",
      "                Second outer paragraph.\n",
      "            </b>\n",
      "</p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('p', class_=\"outer-text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                First outer paragraph.\n",
      "            </b>\n",
      "</p>, <p class=\"outer-text\">\n",
      "<b>\n",
      "                Second outer paragraph.\n",
      "            </b>\n",
      "</p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(class_=\"outer-text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(id = \"first\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CSS Selectors\n",
    "\n",
    "1. body p a : find all a tags inside p tag inside body tag.\n",
    "2. p.outer-text : find all p tags with a **class** of outer-text.\n",
    "3. p\\#first : find all p tags with an **id** of first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p>, <p class=\"inner-text\">\n",
      "                Second paragraph.\n",
      "            </p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.select(\"div p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"tombstone-container\">\n",
      " <p class=\"period-name\">\n",
      "  Overnight\n",
      "  <br/>\n",
      "  <br/>\n",
      " </p>\n",
      " <p>\n",
      "  <img alt=\"Overnight: A 50 percent chance of showers.  Mostly cloudy early, then becoming mostly clear, with a low around 48. West wind around 8 mph.  New precipitation amounts of less than a tenth of an inch possible. \" class=\"forecast-icon\" src=\"newimages/medium/nshra50.png\" title=\"Overnight: A 50 percent chance of showers.  Mostly cloudy early, then becoming mostly clear, with a low around 48. West wind around 8 mph.  New precipitation amounts of less than a tenth of an inch possible. \"/>\n",
      " </p>\n",
      " <p class=\"short-desc\">\n",
      "  Chance\n",
      "  <br/>\n",
      "  Showers\n",
      " </p>\n",
      " <p class=\"temp temp-low\">\n",
      "  Low: 48 °F\n",
      " </p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "seven_day = soup.find(id=\"seven-day-forecast\")\n",
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "tonight = forecast_items[0]\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overnight\n",
      "ChanceShowers\n",
      "Low: 48 °F\n"
     ]
    }
   ],
   "source": [
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "print(period)\n",
    "print(short_desc)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overnight: A 50 percent chance of showers.  Mostly cloudy early, then becoming mostly clear, with a low around 48. West wind around 8 mph.  New precipitation amounts of less than a tenth of an inch possible. \n"
     ]
    }
   ],
   "source": [
    "img = tonight.find(\"img\")\n",
    "desc = img['title']\n",
    "print(desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
